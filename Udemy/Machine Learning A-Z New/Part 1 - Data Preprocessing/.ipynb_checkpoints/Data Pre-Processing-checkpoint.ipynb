{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing The Essential Libraries\n",
    "\n",
    "Libraries are set of tools which can be used to perform specific functions. Like for example statistical function of mean. We import a library which gives the function of mean and we use that function pass it some data and it then returns the mean of the data.\n",
    "\n",
    "There are many libraries available in Python but there are 3 most commonly used for Data Sciences they are \n",
    "* Numpy - Which stands for Numerical Python - This library contains mathematical tools.\n",
    "* Matplotlib - This library helps us in visualizing the data.\n",
    "* Pandas - This library helps us manage datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing The Essential Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data Set\n",
    "\n",
    "When ever you want to import a data set you need to specify the workinf directory which must be the folder which contains your data set. Easiest way do set a working directory is to save the python file in the location which contains the data set.\n",
    "\n",
    "### NOTE - Python Indexes Start with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0   No\n",
       "1  Yes\n",
       "2   No\n",
       "3   No\n",
       "4  Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing data Set\n",
    "# We will use the pandas library to import the data set.\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "# Now we need to separate the Independent and Dependent variables into their respective data sets\n",
    "X = pd.DataFrame(dataset.iloc[:,:-1].values)\n",
    "y = pd.DataFrame(dataset.iloc[:,-1].values)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented Concepts\n",
    "\n",
    "A class is the model of something we want to build. For example, if we make a house construction plan that gathers the instructions on how to build a house, then this construction plan is the class.\n",
    "\n",
    "An object is an instance of the class. So if we take that same example of the house construction plan, then an object is simply a house. A house (the object) that was built by following the instructions of the construction plan (the class).\n",
    "And therefore there can be many objects of the same class, because we can build many houses from the construction plan.\n",
    "\n",
    "A method is a tool we can use on the object to complete a specific action. So in this same example, a tool can be to open the main door of the house if a guest is coming. A method can also be seen as a function that is applied onto the object, takes some inputs (that were defined in the class) and returns some output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data\n",
    "\n",
    "One of the problems with Real World data is that they contain a lot of missing data points. There are many ways of handling the missing data.\n",
    "\n",
    "* First and the easiest way is to remove the observations which contain missing information. That can be bad in some cases, mostly because along with the missing data we are also loosing other important information which is may be of importance.\n",
    "* Other most common practice is to impute missing values using methodologies. Like for a Continuous Numerical data we can use Mean, for Categorical data we can use Mode etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44</td>\n",
       "      <td>72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30</td>\n",
       "      <td>54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38</td>\n",
       "      <td>61000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40</td>\n",
       "      <td>63777.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>35</td>\n",
       "      <td>58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.7778</td>\n",
       "      <td>52000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>France</td>\n",
       "      <td>48</td>\n",
       "      <td>79000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Germany</td>\n",
       "      <td>50</td>\n",
       "      <td>83000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>France</td>\n",
       "      <td>37</td>\n",
       "      <td>67000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2\n",
       "0   France       44    72000\n",
       "1    Spain       27    48000\n",
       "2  Germany       30    54000\n",
       "3    Spain       38    61000\n",
       "4  Germany       40  63777.8\n",
       "5   France       35    58000\n",
       "6    Spain  38.7778    52000\n",
       "7   France       48    79000\n",
       "8  Germany       50    83000\n",
       "9   France       37    67000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking care of missing data\n",
    "# We will be using a Python Library which will help us Impute missing values into the data set.\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X.iloc[:,1:3])\n",
    "X.iloc[:,1:3] = imputer.transform(X.iloc[:,1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Categorical Data\n",
    "\n",
    "All the machine learning models are mathematical algorithms which need numbers as input for them to run and do not handle Categorical Data well. This is the main reason to encode all the categorical data into numbers so we can apply machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macharya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding Categorical Variables\n",
    "# Python has a predefined library class which will help us perform categorical variable encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X.iloc[:,0] = labelencoder_X.fit_transform(X.iloc[:,0])\n",
    "labelencoder_Y = LabelEncoder()\n",
    "y = labelencoder_Y.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above output we see that the categories of country which are France, Germany and Spain are replaced by 0,1,2 respectively. These should represent numerical values which have mathematical value like 1 is greater than 0 and 2 greater than 1. By imputing with numbers we are artificially inducing some sort of numerical measure to these categories which when used in an ML algorithm will influence the Model. In order to over come this we will use Dummy Variables which basically pivots the categorical values into their own column and each occurance of the variable will have a 1 representing presence and 0 for absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.40000000e+01,\n",
       "        7.20000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 2.70000000e+01,\n",
       "        4.80000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 3.00000000e+01,\n",
       "        5.40000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.80000000e+01,\n",
       "        6.10000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.00000000e+01,\n",
       "        6.37777778e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.50000000e+01,\n",
       "        5.80000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.87777778e+01,\n",
       "        5.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.80000000e+01,\n",
       "        7.90000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 5.00000000e+01,\n",
       "        8.30000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.70000000e+01,\n",
       "        6.70000000e+04]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To create dummy variables we will use another class OneHotEncoder from Sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data Set into Train and Test\n",
    "\n",
    "With any machine learning project we need to split the available data set into Training and Test Data Set.\n",
    "The reason to split the data is so that you have some data for your machine learning model to be validated against, This will help us understand the accuracy of the model and also may be tune the model to perform better or if your model is Over Fitting the training data we can validate it against new data which the model has not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.50000000e+01,\n",
       "        5.80000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.40000000e+01,\n",
       "        7.20000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.80000000e+01,\n",
       "        7.90000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 3.00000000e+01,\n",
       "        5.40000000e+04],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.70000000e+01,\n",
       "        6.70000000e+04],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 4.00000000e+01,\n",
       "        6.37777778e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.80000000e+01,\n",
       "        6.10000000e+04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 3.87777778e+01,\n",
       "        5.20000000e+04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spllitting the data set into train and test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "\n",
    "Lets us consider the data set that we have which has two numerical fields Age and Salary. We can see that these two variables are not in the same scale, Age in the scale of 10's and Salary in the scale of 10000's and this will cause some problems with the machine learning models. This is because most of the machine learning models are based on the concepe of Euclidean Distance. During the calculation of the Euclidean Distance the scale of the variables will have a major impact and since Salary is in a higher scale will have more weightage to that variable.\n",
    "\n",
    "Imagine we are calculating a Euclidean Distance between points (25, 25000) and (50, 50000)\n",
    "Formula = Sqrt((x2^2-x1^2) + (y2^2-y1^2)\n",
    "Formula = sqrt((50^2)-(25^2) + (50000^2) - (25000^2))\n",
    "Formula = sqrt(1875 + 1875000000)\n",
    "Formula = 1875001875\n",
    "\n",
    "We can clearly see that the Salary is dominating the distance calculation. This is the main reason to perform feature scaling so we get the variable in the same scales which will eliminate the effect of larger scale variable on the model.\n",
    "\n",
    "There are several way to scale your data, below are two commonly used methods.\n",
    "\n",
    "![title](Feature_Scaling.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "# Standardization brings all the standardized variables in a scale which is between -1 and +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few questions that may raise is Should we standardize the dummy variables?\n",
    "\n",
    "It's not bad to do, necessarily, but it's not a good habit to get into.  Standardising variables when it's not necessary to do so leaves interpretation issues, and can lead to sloppy thinking.\n",
    "\n",
    "Also, remember that standardisation needs to be applied in the same way to all data sets that are used for a given built model.  Unnecessary standardisation leads to more metadata.  And if someone else uses your model or code without reading carefully, or if you somehow forget...  well, that's not a good thing.  That's true for any variable, true.  But why add unnecessary complications?\n",
    "\n",
    "Next Question which comes to mind is, Is Feature Scaling required for ML Algorithms which do not use Euclidean Distance?\n",
    "\n",
    "Yes, Feature Scaling will help a non Euclidean Model to converge lot faster when using scaled data as opposed to unscaled data.\n",
    "\n",
    "Next question would be that, Should you perform feature scaling to Dependent Variables?\n",
    "\n",
    "No in most cases where the output is categorical or numerical which are variance is not large, But if the output is numerical for example and its range is large we may need to apply feature scaling on dependent variables as well.\n",
    "\n",
    "Another question would be, Why do we Fit and Transform on Training data and only transform on test?\n",
    "\n",
    "https://sebastianraschka.com/faq/docs/scale-training-test.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
