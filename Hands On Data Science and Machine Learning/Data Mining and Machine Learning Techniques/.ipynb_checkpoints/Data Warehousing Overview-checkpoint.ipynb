{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Warehouse Overview\n",
    "\n",
    "Data Warehouse is a Giant data base that contains information from many different sources and its tied together. For example, maybe you work at a big ecommerce company and they might have an ordering system that feeds information about the stuff people bought into your data warehouse.\n",
    "\n",
    "A data warehouse has the challenge of taking data from many different sources, transforming them into some sort of schema that allows us to query these different data sources simultaneously, and it lets us make insights, through data analysis.\n",
    "\n",
    "There are a lot of challenges in data warehousing. One is Data Normalization, You have to figure out how do all these fields in different data sources relate to each other? How do you make sure column from one data source is comparable to column from another data source and has the same set of data, at the same scale, using the same terminology. How do I deal with corrupt data or data from outliers etc. These are all very big challenges and maintaining data feed is also a very big problem.\n",
    "\n",
    "A lot can go wrong when you are importing all this information into your data warehouse. Especially when you have very large transformations that need to happen to take the raw data, into an actual structured database table that can be then imported into the data warehouse. Scaling can also get tricky when you are dealing with monolithic data warehouse. Eventually, your data will get so large that those transformation themselves start to become a problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Versus ELT\n",
    "\n",
    "ETL stands for Extract Transform Load which is the conventional way of doing data warehousing.\n",
    "\n",
    "First you extract data that you want from your operational system. Example, You might extract all web logs from your web server every day. Then that information needs to be transformed into actual structured data base table which can be imported into a data warehouse.\n",
    "\n",
    "This transformation stage might go through every line of those web server logs, transform that into an actual table, Where we are extracting the information line Session ID, What page they looked at, what the time was etc. and then organize that into tabular structure that I can then load into the data ware house itself, as an actual table in a data base. As the data becomes large the transformation step can become a real problem. This can become a scalabilty issue and sometimes can introduce stability problems in the entire data warehouse pipeline.\n",
    "\n",
    "That's where concept of ELT comes in and where we use newer techniques that allow for distributed data bases over a Hadoop Cluster that lets us take advantage of the distributed databases like Hive, Spark or Map Reduce and use that to transform data after the data is loaded.\n",
    "\n",
    "The idea here is that we are going to extract the information we want as we did before, But then we will load the data straight into a data repository, and we will use the repository to actually do the transformation in place.So, the idea here is, instead of doing an offline process to transform my web logs, as an example, into a structured format, we are just going to suck those in as raw text files and go through them one line at a time, using the power of something like Hadoop, to actually transform those into a more structured format that we can then query across entire data warehouse solution.\n",
    "\n",
    "The idea is that instead of using a monolithic database for a data warehouse, you're instead using something built on top of Hadoop, or some sort of a cluster, that can actually not only scale up the processing and querying of that data, but also scale the transformation of that data as well.\n",
    "\n",
    "The overall concept here is that if you move from a monolithic database built on Oracle or MySQL to one of these more modern distributed databases built on top of Hadoop, you can take that transform stage and actually do that after you've loaded in the raw data, as opposed to before. That can end up being simpler and more scalable, and taking advantage of the power of large computing clusters that are available today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
