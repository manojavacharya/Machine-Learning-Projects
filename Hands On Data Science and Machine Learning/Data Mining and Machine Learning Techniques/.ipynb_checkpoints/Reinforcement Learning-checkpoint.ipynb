{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "We can actually use this idea with an example of Pac- Man. We can actually create a little intelligent Pac-Man agent that can play the game Pac-Man really well on its own.\n",
    "\n",
    "So, the idea behind reinforcement learning is that you have some sort of agent, in this case Pac-Man, that explores some sort of space, and in our example that space will be the maze that Pac-Man is in. As it goes, it learns the value of different state changes within different conditions.\n",
    "\n",
    "![title](PacMan.PNG)\n",
    "\n",
    "From the perceding image, the state of the Agent Pac-Man might be defined by the fact that it has a ghost to the south and a wall to the west and empty spaces to the north and the east and that might be defined as the current state of Pac-Man. The state changes it can take would be to move in a given direction and then we can learn the value of going in a certain direction. For example, if the agent were to move to the north or east nothing would happen, there is no real reward associated with it. But if you were to move south you would be destroyed by the Ghost and that would be a negative value.\n",
    "\n",
    "As we go explore the entire space, we can build up a set of all possible states the Pac-Man can be in, and the values associated with moving in a given direction in each one of those states. And that's Reinforcemennt Learning. And as it explores the whole space, it refined these reward values for a given state and it can then use these stored reward values to make the best decision at a given current set of condition or state.\n",
    "\n",
    "The benefit of this technique is that once you've explored the entire set of possible states that your agent can be in, you can very quickly have a very good performance when you run different iterations of this. So, you know, you can basically make an intelligent Pac-Man by running reinforcement learning and letting it explore the values of different decisions it can make in different states and then storing that information, to very quickly make the right decision given a future state that it sees in an unknown set of conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learning\n",
    "\n",
    "A very Specific Implementation of Reinforcement Learning is Q-Learning.\n",
    "\n",
    "* So again, you start with a set of environmental states of the agent (Is there a ghost next to me? Is there a power pill in front of me? Things like that.), we're going to call that s.\n",
    "* I have a set of possible actions that I can take in those states, we're going to call that set of actions a. In the case of Pac-Man, those possible actions are move up, down, left, or right.\n",
    "* Then we have a value for each state/action pair that we'll call Q; that's why we call it Q-learning. So, for each state, a given set of conditions surrounding Pac-Man, a given action will have a value Q. So, moving up might have a given value Q, moving down might have a negative Q value if it means encountering a ghost, for example.\n",
    "\n",
    "So, we start off with a Q value of 0 for every possible state that Pac-Man could be in. And, as Pac-Man explores a maze, as bad things happen to Pac-Man, we reduce the Q value for the state that Pac-Man was in at the time. So, if Pac-Man ends up getting eaten by a ghost, we penalize whatever he did in that current state. As good things happen to Pac-Man, as he eats a power pill, or eats a ghost, we'll increase the Q value for that action, for the state that he was in. Then, what we can do is use those Q values to inform Pac-Man's future choices, and sort of build a little intelligent agent that can perform optimally, and make a perfect little Pac-Man. From the same image of Pac-Man that we saw just above, we can further define the current state of Pac-Man by defining that he has a wall to the West, empty space to the North and East, a ghost to the South.\n",
    "\n",
    "We can look at the actions he can take: he can't actually move left at all, but he can move up, down, or right, and we can assign a value to all those actions. By going up or right, nothing really happens at all, there's no power pill or dots to consume. But if he goes left, that's definitely a negative value. We can say for the state given by the current conditions that Pac-Man is surrounded by, moving down would be a really bad choice; there should be a negative Q value for that. Moving left just can't be done at all. Moving up or right or staying neutral, the Q value would remain 0 for those action choices for that given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Exploration Problem.\n",
    "\n",
    "One problem that we have in reinforcement learning is the exploration problem. How do I make sure that I efficiently cover all the different states and actions within those states during the exploration phase?\n",
    "\n",
    "There are many different approaches to address this.\n",
    "\n",
    "* One simple approach is to always choose the action for a given state with the highest Q value that was computed so far, and if there is a tie then choose at random. So, initially when all your Q values are 0 at the start then you pic an action at random. As we gain information about better Q values for actions and given states, we will start to use those as we go. But that ends up being inefficient and we can actually miss a lot of paths that way if we just tie ourselves to a rigid algorithm of always choosing the best Q value computed so far.\n",
    "\n",
    "* A better way is to introduce a little bit of random variation into your actions, which we call it as epsilon term. We set an epsilon value initially and suppose you roll the dice, I get a random number, if it ends up being less than epislon value then you take a random path try it our and see what the result is. That actually lets you explore much wider range of possibilities, much wider range of actions, for wider range of states more efficiently during the exploration stage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "We can explore some set of actions that we can take for a given set of states, we use that to inform the rewards associated with a given action for a given set of states, and after that exploration is done we can use that information, those Q values, to intelligently navigate through an entirely new maze for example. This can also be called a Markov decision process.\n",
    "\n",
    "Markov Decision Process - \"a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "It is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions ideally, using a memory-based data structure. The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
